2/1/2013
The AdaBoostR code is complete - at least this first version. Cannot do much in the way of testing until the weak
learner specified by abstract class WeakLearner is built.
The AdaBoost algorithm is outlined more or less in the comments of the code.
I added a demarcation bound to weed out terrible learners. Not sure what to set parameter tau to. Now it is 100.
The combination coefficient of each weak learner is set within the iteration that the learner is added and not modified
afterward.

1/31/2013
Did some work on the high-level AdaBoost framework in AdaBoostR.java. Wrote the functions evaluateCost and minimizeCost to
take care of calculating combination coefficient of new learner. I am not sure if combination coefficient has to be
reevaluated for EVERY learner in the committee on every iteration or if it is just evaluated once when a learner is added
to the system.

Last week, Professor Torresani proposed the following algorithm for a WeakLearner instance:
1. Choose random subset of features St such that |St| = some constant k.
2. For each example x(i), build a quadratic basis vector phi using St.
3. Regularized least square regression to compute learning hypothesis.

The next step after completing high level AdaBoostR is to implement this as weak learners and try to generate hundreds of 
unique weak learners.